{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Computational Linear Algebra: Singular Value Decomposition Homework\n",
        "\n",
        "In the following homework we decided to explore the topic of *Singular Value Decomposition* used to device a Movie recommendation system like the one used nowdays by many streaming services."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Dataset Specifications\n",
        "The dataset we decided to use is the *MovieLens Dataset* which is one of the most widely used datasets for movie recommendation tasks. Such dataset contains user ratings for movies along with metadata like movie genres, titles, and timestamps.\n",
        "\n",
        "In particular, we considered the \"MovieLens 1M Dataset\", which contains 1 million ratings from 6000 users on 4000 movies. The dataset is divided into 3 main files:\n",
        "- \"ratings.dat\": which contains all the ratings\n",
        "- \"users.dat\": which contains all the user information\n",
        "- \"movies.dat\": which contains all the movie information\n",
        "\n",
        "### Ratings dataset\n",
        "All ratings are contained in the file \"ratings.dat\" and are in the following format:\n",
        "\n",
        "UserID::MovieID::Rating::Timestamp\n",
        "\n",
        "- UserIDs range between 1 and 6040 \n",
        "- MovieIDs range between 1 and 3952\n",
        "- Ratings are made on a 5-star scale (whole-star ratings only)\n",
        "- Timestamp is represented in seconds since the epoch as returned by time(2)\n",
        "- Each user has at least 20 ratings\n",
        "\n",
        "### Users dataset\n",
        "User information is in the file \"users.dat\" and is in the following format:\n",
        "\n",
        "UserID::Gender::Age::Occupation::Zip-code\n",
        "\n",
        "All demographic information is provided voluntarily by the users and is\n",
        "not checked for accuracy.  Only users who have provided some demographic\n",
        "information are included in this data set.\n",
        "\n",
        "- Gender is denoted by a \"M\" for male and \"F\" for female\n",
        "- Age is chosen from the following ranges:\n",
        "\n",
        "\t*  1:  \"Under 18\"\n",
        "\t* 18:  \"18-24\"\n",
        "\t* 25:  \"25-34\"\n",
        "\t* 35:  \"35-44\"\n",
        "\t* 45:  \"45-49\"\n",
        "\t* 50:  \"50-55\"\n",
        "\t* 56:  \"56+\"\n",
        "\n",
        "- Occupation is chosen from the following choices:\n",
        "\n",
        "\t*  0:  \"other\" or not specified\n",
        "\t*  1:  \"academic/educator\"\n",
        "\t*  2:  \"artist\"\n",
        "\t*  3:  \"clerical/admin\"\n",
        "\t*  4:  \"college/grad student\"\n",
        "\t*  5:  \"customer service\"\n",
        "\t*  6:  \"doctor/health care\"\n",
        "\t*  7:  \"executive/managerial\"\n",
        "\t*  8:  \"farmer\"\n",
        "\t*  9:  \"homemaker\"\n",
        "\t* 10:  \"K-12 student\"\n",
        "\t* 11:  \"lawyer\"\n",
        "\t* 12:  \"programmer\"\n",
        "\t* 13:  \"retired\"\n",
        "\t* 14:  \"sales/marketing\"\n",
        "\t* 15:  \"scientist\"\n",
        "\t* 16:  \"self-employed\"\n",
        "\t* 17:  \"technician/engineer\"\n",
        "\t* 18:  \"tradesman/craftsman\"\n",
        "\t* 19:  \"unemployed\"\n",
        "\t* 20:  \"writer\"\n",
        "\n",
        "### Movies dataset\n",
        "Movie information is in the file \"movies.dat\" and is in the following\n",
        "format:\n",
        "\n",
        "MovieID::Title::Genres\n",
        "\n",
        "- Titles are identical to titles provided by the IMDB (including\n",
        "year of release)\n",
        "- Genres are pipe-separated and are selected from the following genres:\n",
        "\n",
        "\t* Action\n",
        "\t* Adventure\n",
        "\t* Animation\n",
        "\t* Children's\n",
        "\t* Comedy\n",
        "\t* Crime\n",
        "\t* Documentary\n",
        "\t* Drama\n",
        "\t* Fantasy\n",
        "\t* Film-Noir\n",
        "\t* Horror\n",
        "\t* Musical\n",
        "\t* Mystery\n",
        "\t* Romance\n",
        "\t* Sci-Fi\n",
        "\t* Thriller\n",
        "\t* War\n",
        "\t* Western\n",
        "\n",
        "- Some MovieIDs do not correspond to a movie due to accidental duplicate\n",
        "entries and/or test entries\n",
        "- Movies are mostly entered by hand, so errors and inconsistencies may exist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2 Singular Value Decomposition (SVD) - Theory\n",
        "\n",
        "SVD is a matrix factorization technique for a matrix $A \\in \\mathbb{C}^{m \\times n}$, expressed as:\n",
        "\n",
        "$$\n",
        "A = U \\Sigma V^H\n",
        "$$\n",
        "\n",
        "where:\n",
        "- $U$ is an orthogonal $m \\times m$ matrix with left singular vectors,\n",
        "- $\\Sigma$ is a diagonal $m \\times n$ matrix with non-negative singular values,\n",
        "- $V$ is an orthogonal $n \\times n$ matrix with right singular vectors.\n",
        "\n",
        "Key properties:\n",
        "- The columns of $U$ are eigenvectors of $A A^T$.\n",
        "- The columns of $V$ are eigenvectors of $A^T A$.\n",
        "- The diagonal elements of $\\Sigma$ are the singular values, corresponding to the square roots of the eigenvalues of $A A^T$ or $A^T A$.\n",
        "If $A \\in \\mathbb{R}^{m \\times n}$, the decomposition becomes: $A = U \\Sigma V^T$.\n",
        "\n",
        "### Geometric Interpretation\n",
        "\n",
        "The SVD represents a matrix transformation as a sequence of three operations:\n",
        "1. **Rotation by $V^T$**: Aligns the input space with the right singular vectors.\n",
        "2. **Scaling by $\\Sigma$**: Scales along axes defined by the singular vectors.\n",
        "3. **Rotation by $U$**: Rotates the output space to align with the left singular vectors.\n",
        "\n",
        "This decomposition helps in understanding and visualizing linear transformations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
        "\n",
        "# Define face colors for consistency\n",
        "face_colors = [\n",
        "    (1, 0, 0, 0.6),  # Red\n",
        "    (0, 1, 0, 0.6),  # Green\n",
        "    (0, 0, 1, 0.6),  # Blue\n",
        "    (1, 1, 0, 0.6),  # Yellow\n",
        "    (1, 0, 1, 0.6),  # Magenta\n",
        "    (0, 1, 1, 0.6)   # Cyan\n",
        "]\n",
        "\n",
        "# Function to generate a 3D parallelepiped (unit cube)\n",
        "def generate_parallelepiped():\n",
        "    vertices = np.array([\n",
        "        [0, 0, 0], [1, 0, 0], [1, 1, 0], [0, 1, 0],  # Bottom face\n",
        "        [0, 0, 1], [1, 0, 1], [1, 1, 1], [0, 1, 1]   # Top face\n",
        "    ])\n",
        "    return vertices - 0.5  # Center the parallelepiped\n",
        "\n",
        "# Function to apply a linear transformation to vertices\n",
        "def apply_transformation(vertices, matrix):\n",
        "    return np.dot(vertices, matrix.T)\n",
        "\n",
        "# Function to plot a colored parallelepiped in 3D\n",
        "def plot_parallelepiped_3d(ax, vertices, title):\n",
        "    faces = [\n",
        "        [vertices[0], vertices[1], vertices[2], vertices[3]],  # Bottom face\n",
        "        [vertices[4], vertices[5], vertices[6], vertices[7]],  # Top face\n",
        "        [vertices[0], vertices[1], vertices[5], vertices[4]],  # Side face\n",
        "        [vertices[2], vertices[3], vertices[7], vertices[6]],  # Side face\n",
        "        [vertices[1], vertices[2], vertices[6], vertices[5]],  # Side face\n",
        "        [vertices[4], vertices[7], vertices[3], vertices[0]]   # Side face\n",
        "    ]\n",
        "    poly3d = Poly3DCollection(faces, facecolors=face_colors, linewidths=1, edgecolors='k')\n",
        "    ax.add_collection3d(poly3d)\n",
        "\n",
        "    ax.set_title(title, fontsize=10)\n",
        "    ax.set_xlim([-1, 1])\n",
        "    ax.set_ylim([-1, 1])\n",
        "    ax.set_zlim([-1, 1])\n",
        "    ax.view_init(elev=20, azim=30)  # Rotate POV slightly\n",
        "    ax.set_xlabel(\"X\")\n",
        "    ax.set_ylabel(\"Y\")\n",
        "    ax.set_zlabel(\"Z\")\n",
        "\n",
        "# Function to plot a transformation in 2D with colors\n",
        "def plot_parallelepiped_2d(ax, vertices, title):\n",
        "    ax.set_title(title, fontsize=10)\n",
        "    ax.set_xlim([-2, 2])\n",
        "    ax.set_ylim([-2, 2])\n",
        "    ax.set_xlabel(\"X\")\n",
        "    ax.set_ylabel(\"Y\")\n",
        "\n",
        "    # Get only the first two dimensions for 2D visualization\n",
        "    vertices_2d = vertices[:, :2]\n",
        "\n",
        "    # Define corresponding faces for 2D view\n",
        "    faces_2d = [\n",
        "        [vertices_2d[0], vertices_2d[1], vertices_2d[2], vertices_2d[3]],  # Bottom face\n",
        "        [vertices_2d[4], vertices_2d[5], vertices_2d[6], vertices_2d[7]],  # Top face\n",
        "        [vertices_2d[0], vertices_2d[1], vertices_2d[5], vertices_2d[4]],  # Side face\n",
        "        [vertices_2d[2], vertices_2d[3], vertices_2d[7], vertices_2d[6]],  # Side face\n",
        "        [vertices_2d[1], vertices_2d[2], vertices_2d[6], vertices_2d[5]],  # Side face\n",
        "        [vertices_2d[4], vertices_2d[7], vertices_2d[3], vertices_2d[0]]   # Side face\n",
        "    ]\n",
        "\n",
        "    # Plot each face with the corresponding color\n",
        "    for face, color in zip(faces_2d, face_colors):\n",
        "        face.append(face[0])  # Close the polygon\n",
        "        face = np.array(face)\n",
        "        ax.fill(face[:, 0], face[:, 1], color=color, edgecolor='k', linewidth=1)\n",
        "\n",
        "# Define an arbitrary transformation matrix (3x3 for mapping R3 to R3)\n",
        "A = np.array([[1, 2, 1], [0, 1, -1], [1, 0, 1]])\n",
        "\n",
        "# Perform SVD decomposition\n",
        "U, S, Vt = np.linalg.svd(A, full_matrices=False)\n",
        "\n",
        "# Matrices involved in the transformation\n",
        "rotation1 = Vt  # First rotation (Right singular vectors)\n",
        "scaling = np.diag(S)  # Scaling matrix (Singular values)\n",
        "rotation2 = U  # Second rotation (Left singular vectors)\n",
        "\n",
        "# Original parallelepiped\n",
        "vertices = generate_parallelepiped()\n",
        "\n",
        "# Apply transformations\n",
        "vertices_rot1 = apply_transformation(vertices, rotation1)  # Rotate (3D)\n",
        "vertices_projected = vertices_rot1[:, :2]  # Dimension Reduction (Drop Z)\n",
        "vertices_stretch = apply_transformation(vertices_projected, scaling[:2, :2])  # Stretch\n",
        "vertices_rot2 = apply_transformation(vertices_stretch, rotation2[:2, :2])  # Final Rotate\n",
        "\n",
        "# Plot results: 3D rotation, dimension reduction, 2D stretch, 2D final rotation\n",
        "fig = plt.figure(figsize=(16, 4))\n",
        "\n",
        "# First plot (3D): Rotate by right singular vectors\n",
        "ax1 = fig.add_subplot(141, projection='3d')\n",
        "plot_parallelepiped_3d(ax1, vertices_rot1, \"Rotate (Right Singular Vectors)\")\n",
        "\n",
        "# Second plot (2D): Dimension Reduction (Projection to 2D)\n",
        "ax2 = fig.add_subplot(142)\n",
        "plot_parallelepiped_2d(ax2, vertices_projected, \"Dimension Erase\")\n",
        "\n",
        "# Third plot (2D): Stretch by singular values\n",
        "ax3 = fig.add_subplot(143)\n",
        "plot_parallelepiped_2d(ax3, vertices_stretch, \"Stretch (Singular Values)\")\n",
        "\n",
        "# Fourth plot (2D): Rotate by left singular vectors\n",
        "ax4 = fig.add_subplot(144)\n",
        "plot_parallelepiped_2d(ax4, vertices_rot2, \"Rotate (Left Singular Vectors)\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Dataset preparation\n",
        "### 2.3 Loading the separate datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ratings:\n",
            "   UserID  MovieID  Rating  Timestamp\n",
            "0       1     1193       5  978300760\n",
            "1       1      661       3  978302109\n",
            "2       1      914       3  978301968\n",
            "3       1     3408       4  978300275\n",
            "4       1     2355       5  978824291\n",
            "\n",
            "Movies:\n",
            "   MovieID                               Title                        Genres\n",
            "0        1                    Toy Story (1995)   Animation|Children's|Comedy\n",
            "1        2                      Jumanji (1995)  Adventure|Children's|Fantasy\n",
            "2        3             Grumpier Old Men (1995)                Comedy|Romance\n",
            "3        4            Waiting to Exhale (1995)                  Comedy|Drama\n",
            "4        5  Father of the Bride Part II (1995)                        Comedy\n",
            "\n",
            "Users:\n",
            "   UserID Gender  Age  Occupation Zip-code\n",
            "0       1      F    1          10    48067\n",
            "1       2      M   56          16    70072\n",
            "2       3      M   25          15    55117\n",
            "3       4      M   45           7    02460\n",
            "4       5      M   25          20    55455\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import wordcloud\n",
        "\n",
        "# Load ratings.dat\n",
        "ratings = pd.read_csv(\n",
        "    'MovieLens1M/ratings.dat', \n",
        "    sep='::', \n",
        "    engine='python', \n",
        "    names=['UserID', 'MovieID', 'Rating', 'Timestamp'],\n",
        "    encoding='ISO-8859-1'\n",
        ")\n",
        "\n",
        "# Load movies.dat\n",
        "movies = pd.read_csv(\n",
        "    'MovieLens1M/movies.dat', \n",
        "    sep='::', \n",
        "    engine='python', \n",
        "    names=['MovieID', 'Title', 'Genres'],\n",
        "    encoding='ISO-8859-1'\n",
        ")\n",
        "\n",
        "# Load users.dat\n",
        "users = pd.read_csv(\n",
        "    'MovieLens1M/users.dat', \n",
        "    sep='::', \n",
        "    engine='python', \n",
        "    names=['UserID', 'Gender', 'Age', 'Occupation', 'Zip-code'],\n",
        "    encoding='ISO-8859-1'\n",
        ")\n",
        "\n",
        "# Display the first few rows\n",
        "print(\"Ratings:\")\n",
        "print(ratings.head())\n",
        "print(\"\\nMovies:\")\n",
        "print(movies.head())\n",
        "print(\"\\nUsers:\")\n",
        "print(users.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Merge DataFrames\n",
        "We’ll merge the ratings, movies, and users DataFrames to create a single dataset for analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Merged Data:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UserID</th>\n",
              "      <th>MovieID</th>\n",
              "      <th>Rating</th>\n",
              "      <th>Timestamp</th>\n",
              "      <th>Title</th>\n",
              "      <th>Genres</th>\n",
              "      <th>Gender</th>\n",
              "      <th>Age</th>\n",
              "      <th>Occupation</th>\n",
              "      <th>Zip-code</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1193</td>\n",
              "      <td>5</td>\n",
              "      <td>978300760</td>\n",
              "      <td>One Flew Over the Cuckoo's Nest (1975)</td>\n",
              "      <td>Drama</td>\n",
              "      <td>F</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>48067</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>661</td>\n",
              "      <td>3</td>\n",
              "      <td>978302109</td>\n",
              "      <td>James and the Giant Peach (1996)</td>\n",
              "      <td>Animation|Children's|Musical</td>\n",
              "      <td>F</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>48067</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>914</td>\n",
              "      <td>3</td>\n",
              "      <td>978301968</td>\n",
              "      <td>My Fair Lady (1964)</td>\n",
              "      <td>Musical|Romance</td>\n",
              "      <td>F</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>48067</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>3408</td>\n",
              "      <td>4</td>\n",
              "      <td>978300275</td>\n",
              "      <td>Erin Brockovich (2000)</td>\n",
              "      <td>Drama</td>\n",
              "      <td>F</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>48067</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>2355</td>\n",
              "      <td>5</td>\n",
              "      <td>978824291</td>\n",
              "      <td>Bug's Life, A (1998)</td>\n",
              "      <td>Animation|Children's|Comedy</td>\n",
              "      <td>F</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>48067</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   UserID  MovieID  Rating  Timestamp                                   Title  \\\n",
              "0       1     1193       5  978300760  One Flew Over the Cuckoo's Nest (1975)   \n",
              "1       1      661       3  978302109        James and the Giant Peach (1996)   \n",
              "2       1      914       3  978301968                     My Fair Lady (1964)   \n",
              "3       1     3408       4  978300275                  Erin Brockovich (2000)   \n",
              "4       1     2355       5  978824291                    Bug's Life, A (1998)   \n",
              "\n",
              "                         Genres Gender  Age  Occupation Zip-code  \n",
              "0                         Drama      F    1          10    48067  \n",
              "1  Animation|Children's|Musical      F    1          10    48067  \n",
              "2               Musical|Romance      F    1          10    48067  \n",
              "3                         Drama      F    1          10    48067  \n",
              "4   Animation|Children's|Comedy      F    1          10    48067  "
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Merge ratings with movies\n",
        "ratings_movies = pd.merge(ratings, movies, on='MovieID')\n",
        "\n",
        "# Merge the result with users\n",
        "full_data = pd.merge(ratings_movies, users, on='UserID')\n",
        "\n",
        "# Display the merged dataset\n",
        "print(\"Merged Data:\")\n",
        "full_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Preprocessing\n",
        "In order to apply the SVD decomposition we need to preprocess the data in the dataset.\n",
        "### 3.1 Normalize ratings\n",
        "We normalize the ratings to ensure fair comparison across users."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Normalized Ratings (mean=-4.7489856505553035e-17, std=1.000000499895897):\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Rating</th>\n",
              "      <th>Normalized_Rating</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5</td>\n",
              "      <td>1.269747</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>-0.520601</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>-0.520601</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>0.374573</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>1.269747</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Rating  Normalized_Rating\n",
              "0       5           1.269747\n",
              "1       3          -0.520601\n",
              "2       3          -0.520601\n",
              "3       4           0.374573\n",
              "4       5           1.269747"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Normalize the ratings\n",
        "scaler = StandardScaler()\n",
        "full_data['Normalized_Rating'] = scaler.fit_transform(full_data[['Rating']])\n",
        "\n",
        "print(f\"Normalized Ratings (mean={full_data['Normalized_Rating'].mean()}, std={full_data['Normalized_Rating'].std()}):\")\n",
        "full_data[['Rating', 'Normalized_Rating']].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Data Matrix\n",
        "In order to apply SVD we transform the dataset into a  user-item matrix where rows are users, columns are movies, and values are ratings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy.sparse.linalg import svds\n",
        "from sklearn.impute import SimpleImputer\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from scipy.cluster.hierarchy import linkage, leaves_list\n",
        "\n",
        "# Handle the missing values by filling with the mean rating for each user\n",
        "user_item_matrix = full_data.pivot(index='UserID', columns='MovieID', values='Normalized_Rating')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since user-movie rating matrices are inherently sparse, handling missing values properly is crucial. We've observed that in general users tend to rate movies based on personal biases. Therefore, we decided to handle the missing values by user mean imputation to preserve their individual rating tendencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User-Item Matrix Shape: (6040, 3706)\n"
          ]
        }
      ],
      "source": [
        "imp = SimpleImputer(strategy='mean')\n",
        "user_item_matrix = pd.DataFrame(imp.fit_transform(user_item_matrix.T).T, index=user_item_matrix.index, columns=user_item_matrix.columns)\n",
        "\n",
        "# Convert to numpy array\n",
        "matrix = user_item_matrix.values\n",
        "\n",
        "# Display the matrix shape\n",
        "print(\"User-Item Matrix Shape:\", matrix.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In order to device a more impactful procedure to predict the correct recomendations, we decided to sort the users in the user-item matrix so that the users with higher *cosine similarity* are adjacent:\n",
        "$$\n",
        "cos(x,y)=\\frac{x\\cdot y}{||x||||y||}\n",
        "$$\n",
        "To do so, we performed hierarchical clustering on the users and selected the subgroups produced to create such ordering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute cosine similarity between users\n",
        "user_similarity = cosine_similarity(user_item_matrix)\n",
        "\n",
        "# Perform hierarchical clustering\n",
        "\n",
        "# Use average linkage method for clustering\n",
        "linkage_matrix = linkage(user_similarity, method='average')\n",
        "\n",
        "# Get the order of users based on clustering\n",
        "user_order = leaves_list(linkage_matrix)\n",
        "\n",
        "# Reorder the user-item matrix based on the clustering\n",
        "user_item_matrix = user_item_matrix.iloc[user_order]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Perform SVD\n",
        "We preform Singular Value Decomposition on the user-item matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def bidiagonal(A):\n",
        "    m, n = A.shape\n",
        "    B = A.copy()  # Make a copy of A for modification\n",
        "    P = np.eye(m)  # Initialize the orthogonal matrix P\n",
        "    H = np.eye(n)  # Initialize the orthogonal matrix H\n",
        "    \n",
        "    for k in range(n):\n",
        "        # Construct Householder vector for column k\n",
        "        a = B[k:, k].copy()\n",
        "        a[0] += np.sign(a[0]) * np.linalg.norm(a)\n",
        "        P_tilde = np.eye(m)\n",
        "        P_tilde[k:, k:] -= 2 * np.outer(a, a) / np.dot(a, a)  # Outer product instead of dot product\n",
        "        \n",
        "        # Update matrix B and P\n",
        "        B = P_tilde @ B\n",
        "        P = P_tilde @ P\n",
        "        \n",
        "        if k < n - 1:  # Only need to reflect in the row direction for k < n-1\n",
        "            # Construct Householder vector for row k\n",
        "            b = B[k, k+1:].copy()\n",
        "            b[0] += np.sign(b[0]) * np.linalg.norm(b)\n",
        "            H_tilde = np.eye(n)\n",
        "            H_tilde[k+1:, k+1:] -= 2 * np.outer(b, b) / np.dot(b, b)  # Outer product instead of dot product\n",
        "            \n",
        "            # Update matrix B and H\n",
        "            B = B @ H_tilde\n",
        "            H = H @ H_tilde\n",
        "    \n",
        "    return P, B, H\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy.linalg import qr\n",
        "from numpy.linalg import eig\n",
        "def my_svd(A):\n",
        "    _, B , H = bidiagonal(A)\n",
        "    \n",
        "    # Compute eigenvalues and eigenvectors of B^T B\n",
        "    eigvals, Q_tilde = eig(np.dot(B.T, B))\n",
        "    \n",
        "    # Sort the eigenvalues in descending order and order Q_tilde accordingly\n",
        "    sorted_indices = np.argsort(-eigvals)  # Sort in descending order\n",
        "    eigvals = np.sqrt(np.abs(eigvals[sorted_indices]))  # Singular values (square root of eigenvalues)\n",
        "    Q_tilde = Q_tilde[:, sorted_indices]\n",
        "    \n",
        "    # Compute the right singular vectors\n",
        "    V = Q_tilde\n",
        "    \n",
        "    # Compute the left singular vectors\n",
        "    Q = H @ Q_tilde\n",
        "    \n",
        "    # Construct the diagonal matrix S with singular values\n",
        "    S = np.diag(eigvals)\n",
        "    \n",
        "    # Compute the C matrix (A @ V)\n",
        "    C = A @ V\n",
        "    \n",
        "    # Perform QR decomposition to obtain U and R\n",
        "    U, R, P = qr(C, mode='economic', pivoting=True)\n",
        "    \n",
        "    # Ensure that U, S, and V are correct\n",
        "    V = V @ P\n",
        "    S = R\n",
        "    \n",
        "    return U, S, V\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "A = np.array([[1,2,3], [4,5,6], [7,8,7], [4,2,3]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([-1.72082795, -0.28251108,  4.45856543,  1.36415016])"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "U, S, V = my_svd(A)\n",
        "U@S@V.T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([[-0.16912369, -0.67383391, -0.01584365],\n",
              "        [-0.48961621, -0.57566278,  0.03921623],\n",
              "        [-0.78422289,  0.41591354, -0.41659194],\n",
              "        [-0.3415664 ,  0.20390232,  0.90810915]]),\n",
              " array([[14.43820248, -7.38259038,  3.44331543],\n",
              "        [ 0.        , -1.98527771,  0.82138783],\n",
              "        [ 0.        ,  0.        ,  1.60101275]]),\n",
              " array([-1.12435938, -1.89584641, -0.37627433]))"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "U,S, V"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### SVD using Deflation Method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def svd_with_deflation(A, num_singular_values=1, maxIter=100, relTol=1e-6):\n",
        "\n",
        "    # Step 1: compute A^TA\n",
        "    ATA = np.dot(A.T, A)\n",
        "    A_current = ATA.copy()\n",
        "\n",
        "    singular_values = []\n",
        "    right_singular_vectors = []\n",
        "    Pmatrices = np.full((num_singular_values, ATA.shape[0], ATA.shape[0]), np.eye(ATA.shape[0]))\n",
        "\n",
        "    # Step 2: Eigenvalues computation with deflation method\n",
        "    for i in range(num_singular_values):\n",
        "        # Slice the current matrix for reduced dimension\n",
        "        A_reduced = A_current.copy()[i:, i:]\n",
        "\n",
        "        # Power iteration to find dominant singular value and vector\n",
        "        singular_value, singular_vector = power_method(A_reduced, np.random.rand(A_reduced.shape[0]), maxIter, relTol)\n",
        "\n",
        "        # Store the singular value\n",
        "        singular_values.append(np.sqrt(singular_value))\n",
        "\n",
        "        # Compute (singular_vector + e_i) / ||singular_vector + e_i|| to get the eigenvector of the original matrix\n",
        "        singular_vector = singular_vector.reshape(-1, 1)\n",
        "        e_i = np.zeros_like(singular_vector)\n",
        "        e_i[0] = 1\n",
        "        u = singular_vector + e_i\n",
        "        \n",
        "        singular_vector = singular_vector.flatten()\n",
        "        if i > 0:\n",
        "            # Transform the singular vector to the original space\n",
        "            original_singular_vector = np.zeros(ATA.shape[0])\n",
        "            original_singular_vector[i:] = singular_vector\n",
        "            P_from_i = np.eye(ATA.shape[0])\n",
        "            for j in range(i):\n",
        "                z = i-j\n",
        "                work_vector = original_singular_vector[z:]\n",
        "                bj = A_current.copy()[z-1, z:]\n",
        "                original_singular_vector[z-1] = (bj.T @ Pmatrices[z][z:,z:] @ work_vector)/(singular_values[i] - singular_values[z-1])\n",
        "                P_from_i = Pmatrices[z] @ P_from_i\n",
        "            P_from_i = Pmatrices[0] @ P_from_i\n",
        "            singular_vector = P_from_i @ original_singular_vector\n",
        "        right_singular_vectors.append(singular_vector)\n",
        "\n",
        "        # Compute the P_{i+1} matrix\n",
        "        Pi = np.eye(A_current.shape[0])\n",
        "        Pi[i:, i:] -= 2 * np.outer(u, u.T)/(np.linalg.norm(u)**2)\n",
        "        Pmatrices[i] = Pi\n",
        "\n",
        "        # Update A_current with deflation step A_current = P_i A_current P_i^T\n",
        "        A_current = Pi @ A_current @ Pi.T\n",
        "\n",
        "    # Step 3: Store the singular values and right singular vectors\n",
        "    Sigma = np.array(singular_values)\n",
        "    Vt = np.array(right_singular_vectors)\n",
        "\n",
        "    # Step 4: Compute the left singular vectors\n",
        "    U = np.zeros((A.shape[0], num_singular_values))\n",
        "    for i in range(num_singular_values):\n",
        "        U[:, i] = A @ Vt[i, :] / Sigma[i]\n",
        "\n",
        "    return U, np.diag(Sigma), Vt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### SVD using Shifting Method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def svd_with_shift(A, num_singular_values=1, maxIter=100, relTol=1e-6):\n",
        "\n",
        "    # Step 1: compute A^TA\n",
        "    ATA = np.dot(A.T, A)\n",
        "\n",
        "    singular_values = []\n",
        "    right_singular_vectors = []\n",
        "\n",
        "    # Step 2: Eigenvalues computation with deflation method\n",
        "    for _ in range(num_singular_values):\n",
        "        # Power iteration to find dominant singular value and vector\n",
        "        singular_value, singular_vector = power_method(ATA, np.random.rand(ATA.shape[1]), maxIter, relTol)\n",
        "\n",
        "        # Store the singular value and vector\n",
        "        singular_values.append(np.sqrt(singular_value))\n",
        "        right_singular_vectors.append(singular_vector)\n",
        "\n",
        "        # Deflation step\n",
        "        ATA = ATA - singular_value * np.outer(singular_vector, singular_vector)\n",
        "\n",
        "    # Step 3: Store the singular values and right singular vectors\n",
        "    Sigma = np.array(singular_values)\n",
        "    Vt = np.array(right_singular_vectors)\n",
        "\n",
        "    # Step 4: Compute the left singular vectors\n",
        "    U = np.zeros((A.shape[0], num_singular_values))\n",
        "    for i in range(num_singular_values):\n",
        "        U[:, i] = A @ Vt[i, :] / Sigma[i]\n",
        "\n",
        "    return U, np.diag(Sigma), Vt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Deflation vs Shifting comparison\n",
        "We've analyzed the complexity of these two algorithms and showed that:\n",
        "- **Deflation Method Complexity**: $$O(k^2 \\cdot n^2)$$\n",
        "- **Shifting Method Complexity**: $$O(k \\cdot n^2)$$\n",
        "Below, in the code, we run a quick demonstration of the validity of our assumptions. We use the svds method provided by scipy as reference to compare the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "U, S, Vt = svds(matrix, k=10)\n",
        "# Sort the singular values in descending order\n",
        "sorted_indices = np.argsort(S)[::-1]  # Indices for sorting in descending order\n",
        "S = S[sorted_indices]           # Sort singular values\n",
        "U = U[:, sorted_indices]                # Reorder columns of U\n",
        "Vt = Vt[sorted_indices, :]              # Reorder rows of V^T\n",
        "\n",
        "start_deflation = time.time()\n",
        "U_deflation, S_deflation, Vt_deflation = svd_with_deflation(matrix, num_singular_values=10)\n",
        "end_deflation = time.time()\n",
        "\n",
        "start_shift = time.time()\n",
        "U_shift, S_shift, Vt_shift = svd_with_shift(matrix, num_singular_values=10)\n",
        "end_shift = time.time()\n",
        "\n",
        "print(f\"Deflation SVD took {end_deflation - start_deflation:.2f} seconds - with MSE: {np.mean((np.diag(S_deflation) - S)**2)}\")\n",
        "print(f\"Shift SVD took {end_shift - start_shift:.2f} seconds - with MSE: {np.mean((np.diag(S_shift) - S)**2)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This clearly shows that the Deflation method is more computationally expensive than the Shifting method. Therefore, we decided to use the second one from this point onward."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2 Selecting the number of Singular Values\n",
        "Singular Value Decomposition (SVD) can be used to reduce data dimensionality, since it provides the best low-rank linear approximation of a given matrix. This can be achieved considering that the magnitude of each singular value represents the \"*importance*\" of its relative component of the decomposition. Dimensionality reduction is achieved by selecting the largest $k$ singular values, where the choice of $k$ depends on the size and structure of the data.\n",
        "\n",
        "Given the SVD decomposition of a matrix $A$ of size $m \\times n$:\n",
        "\n",
        "$$A = U \\Sigma V^T,$$\n",
        "\n",
        "the reduced approximation of $A$ is:\n",
        "\n",
        "$$A_k = U_k \\Sigma_k V_k^T$$\n",
        "\n",
        "where:\n",
        "- $\\Sigma_k$ is a $k \\times k$ matrix, obtained by keeping only the first $k$ singular values,\n",
        "- $U_k$ is an $m \\times k$ matrix, obtained by keeping only the first $k$ columns of $U$,\n",
        "- $V_k^T$ is a $k \\times n$ matrix, obtained by keeping only the first $k$ rows of $V^T$.\n",
        "\n",
        "So, we are required to choose a value of $k$ such that we retain enough information to obtain an accurate prediction and, at the same time, we don't want to choose a value of $k$ too large which would retain too much of the original matrix, defeating the purpose of SVD-based dimensionality reduction.\n",
        "\n",
        "Therefore, we have decided to apply an **elbow method** and choose the value of $k$ in correspondence of the point where the gain in explained variance starts diminishing. This way we can properly balance variance retention and model generalization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "### REAL IMPLEMENTATION WE NEED TO USE\n",
        "U, Sigma, V = my_svd(matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# We plot the significance of the singular values\n",
        "U, sigma, Vt = svd_with_shift(matrix, num_singular_values=500, maxIter=1000, relTol=1e-3)\n",
        "singular_values = np.diag(sigma)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To find a reasonable value $k$ we plot the explained variance of the singular values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "total_variance = np.trace(matrix.T @ matrix)  # Sum of all squared singular values\n",
        "explained_variance_ratio = (singular_values**2) / total_variance\n",
        "cumulative_variance = np.cumsum(explained_variance_ratio)\n",
        "\n",
        "# Plot cumulative explained variance\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(1, len(singular_values ) + 1), cumulative_variance)\n",
        "plt.ylim([0.77, 1])\n",
        "plt.title('Cumulative Explained Variance')\n",
        "plt.xlabel('Number of Singular Values')\n",
        "plt.ylabel('Cumulative Explained Variance')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "k_optimal = 20\n",
        "print(f\"With k = {k_optimal} we explain at least {cumulative_variance[k_optimal-1]*100:.2f}% of the variance\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We derive that 82.3% of the total explained variance is contained in the first 20 singular values. Therefore we select k = 20."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example SVD computation\n",
        "K = 20  # Number of latent features\n",
        "U1, sigma1, Vt1 = svds(matrix, k=K)  # k is the number of latent features\n",
        "\n",
        "# Sort the singular values in descending order\n",
        "sorted_indices = np.argsort(sigma1)[::-1]  # Indices for sorting in descending order\n",
        "sigma1 = sigma1[sorted_indices]           # Sort singular values\n",
        "U1 = U1[:, sorted_indices]                # Reorder columns of U\n",
        "Vt1 = Vt1[sorted_indices, :]              # Reorder rows of V^T\n",
        "\n",
        "# Convert sigma to a diagonal matrix\n",
        "sigma1 = np.diag(sigma1)\n",
        "\n",
        "# Print shapes for verification\n",
        "print(\"U shape:\", U1.shape)        # Should be (rows in matrix, k)\n",
        "print(\"Sigma shape:\", sigma1.shape)  # Should be (k, k)\n",
        "print(\"Vt shape:\", Vt1.shape)      # Should be (k, columns in matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "U = U[:,:K]\n",
        "sigma = sigma[:K,:K]\n",
        "Vt = Vt[:K,:]\n",
        "print(\"U shape:\", U.shape)        # Should be (rows in matrix, k)\n",
        "print(\"Sigma shape:\", sigma.shape)  # Should be (k, k)\n",
        "print(\"Vt shape:\", Vt.shape)\n",
        "print(f\"SVD with shifting + {K} latent features has MSE: {np.mean((np.diag(sigma) - np.diag(sigma1))**2)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From the given decomposition we can reconstruct the original matrix and in doing so predict the missing ratings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reconstruct the predicted matrix as A = U * sigma * Vt\n",
        "predicted_matrix = np.dot(np.dot(U, sigma), Vt)\n",
        "\n",
        "# Convert back to a DataFrame\n",
        "predicted_ratings = pd.DataFrame(predicted_matrix, index=user_item_matrix.index, columns=user_item_matrix.columns)\n",
        "\n",
        "print(\"Predicted Ratings:\")\n",
        "predicted_ratings.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Recommend Movies\n",
        "Based on the matrix of predictions we just computed we can recommend top-rated movies based on predicted ratings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud\n",
        "\n",
        "def plot_genre_wordcloud(user_id, original_data):\n",
        "    # Filter the data for the given user\n",
        "    user_data = original_data[original_data['UserID'] == user_id]\n",
        "\n",
        "    # Calculate the average rating for each genre\n",
        "    genre_ratings = {}\n",
        "    for genres, rating in zip(user_data['Genres'], user_data['Rating']):\n",
        "        for genre in genres.split('|'):\n",
        "            if genre in genre_ratings:\n",
        "                genre_ratings[genre].append(rating)\n",
        "            else:\n",
        "                genre_ratings[genre] = [rating]\n",
        "\n",
        "    # Calculate the average rating for each genre\n",
        "    avg_genre_ratings = {genre: sum(ratings) / len(ratings) for genre, ratings in genre_ratings.items()}\n",
        "\n",
        "    # Generate the word cloud\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(avg_genre_ratings)\n",
        "\n",
        "    # Plot the word cloud\n",
        "    plt.figure(figsize=(7, 5))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    plt.title(f'Genre WordCloud for User {user_id}')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "def recommend_movies(user_id, predicted_ratings, original_data, num_recommendations=5):\n",
        "    user_row = predicted_ratings.loc[user_id].sort_values(ascending=False)\n",
        "\n",
        "    # Exclude movies the user has already rated\n",
        "    rated_movies = original_data[original_data['UserID'] == user_id]['MovieID']\n",
        "    recommendations = user_row[~user_row.index.isin(rated_movies)].head(num_recommendations)\n",
        "\n",
        "    # Map back to movie titles and sort by predicted rating\n",
        "    recommended_movies = movies[movies['MovieID'].isin(recommendations.index)]\n",
        "    recommended_movies = recommended_movies.set_index('MovieID').loc[recommendations.index].reset_index()\n",
        "    return recommended_movies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Recommend movies for a specific user (e.g., user_id = 1)\n",
        "user_id = 2\n",
        "recommended_movies = recommend_movies(user_id, predicted_ratings, full_data, num_recommendations=5)\n",
        "\n",
        "print(f\"The user {user_id} has the following tastes:\")\n",
        "plot_genre_wordcloud(user_id, full_data)\n",
        "print(\"Recommended Movies for User {}:\".format(user_id))\n",
        "recommended_movies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Evaluation\n",
        "//TODO: implement some MSE to evaluate our recomendations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate the model using RMSE\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Filter out NaN values\n",
        "predicted_ratings = predicted_ratings.fillna(0)\n",
        "\n",
        "# Calculate MSE\n",
        "rmse = np.sqrt(mean_squared_error(matrix, predicted_matrix))\n",
        "print(\"RMSE:\", rmse)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ANIME DATASET"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "ratings = pd.read_csv(\n",
        "    'anime/rating.csv', \n",
        "    sep=',', \n",
        ")\n",
        "\n",
        "# Load movies.dat\n",
        "anime = pd.read_csv(\n",
        "    'anime/anime.csv', \n",
        "    usecols=['anime_id', 'name', 'genre'],\n",
        "    sep=',', \n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.merge(ratings, anime, on='anime_id')\n",
        "\n",
        "print(\"Merged Data:\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Deal with missing information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "valid_ratings = df.loc[df['rating'] != -1, 'rating']\n",
        "min_val = valid_ratings.min()\n",
        "max_val = valid_ratings.max()\n",
        "\n",
        "replacement_value = (max_val - min_val) / 2 + 1\n",
        "\n",
        "df['rating'] = df['rating'].replace(-1, replacement_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Deal with duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = df.groupby(['user_id', 'anime_id'], as_index=False).agg({\n",
        "    'rating': 'mean',  # Compute the mean of ratings\n",
        "    'name': 'first',   # Keep the first name (since names should be identical)\n",
        "    'genre': 'first'   # Keep the first genre (since genres should be identical)\n",
        "})\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Normalize the ratings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "df['Normalized_Rating'] = scaler.fit_transform(df[['rating']])\n",
        "\n",
        "print(f\"Normalized Ratings (mean={df['Normalized_Rating'].mean()}, std={df['Normalized_Rating'].std()}):\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "min_val = df['Normalized_Rating'].min()\n",
        "max_val = df['Normalized_Rating'].max()\n",
        "NaN_val = ((max_val - min_val) / 2)+1\n",
        "user_item_matrix = df.pivot(index='user_id', columns='anime_id', values='Normalized_Rating')\n",
        "\n",
        "matrix = user_item_matrix.values\n",
        "\n",
        "# Display the matrix shape\n",
        "print(\"User-Item Matrix Shape:\", matrix.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy.sparse.linalg import svds\n",
        "U, sigma, Vt = svds(matrix, k=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute total variance as the sum of squares of all singular values\n",
        "total_variance = np.trace(matrix.T @ matrix)\n",
        "explained_variance_ratio = (sigma**2) / total_variance\n",
        "cumulative_variance = np.cumsum(explained_variance_ratio)\n",
        "threshold = 0.995  # 99.5% of variance\n",
        "\n",
        "# Plot cumulative explained variance\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(1, len(singular_values) + 1), cumulative_variance, marker='o')\n",
        "plt.title('Cumulative Explained Variance')\n",
        "plt.xlabel('Number of Singular Values')\n",
        "plt.ylabel('Cumulative Explained Variance')\n",
        "plt.axhline(y=threshold, color='r', linestyle='--', label=f\"Threshold at {threshold*100}% variance\")\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Compute the optimal number of singular values to explain at least 99.5% of the variance\n",
        "k_optimal = np.argmax(cumulative_variance >= threshold)+ 1\n",
        "print(f\"Optimal number of singular values to explain at least 99.5% of the variance: {k_optimal}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "U = U[:,:K]\n",
        "sigma = sigma[:K,:K]\n",
        "Vt = Vt[:K,:]\n",
        "print(\"U shape:\", U.shape)        # Should be (rows in matrix, k)\n",
        "print(\"Sigma shape:\", sigma.shape)  # Should be (k, k)\n",
        "print(\"Vt shape:\", Vt.shape)\n",
        "print(f\"SVD with shifting + {K} latent features has MSE: {np.mean((np.diag(sigma) - np.diag(sigma1))**2)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reconstruct the predicted matrix as A = U * sigma * Vt\n",
        "predicted_matrix = np.dot(np.dot(U, sigma), Vt)\n",
        "\n",
        "# Convert back to a DataFrame\n",
        "predicted_ratings = pd.DataFrame(predicted_matrix, index=user_item_matrix.index, columns=user_item_matrix.columns)\n",
        "\n",
        "print(\"Predicted Ratings:\")\n",
        "predicted_ratings.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def recommend_anime(user_id, predicted_ratings, original_data, num_recommendations=5):\n",
        "    user_row = predicted_ratings.loc[user_id].sort_values(ascending=False)\n",
        "\n",
        "    # Exclude movies the user has already rated\n",
        "    rated_anime = original_data[original_data['user_id'] == user_id]['anime_id']\n",
        "    recommendations = user_row[~user_row.index.isin(rated_anime)].head(num_recommendations)\n",
        "\n",
        "    # Map back to movie titles\n",
        "    recommended_anime = anime[anime['anime_id'].isin(recommendations.index)]\n",
        "    return recommended_anime\n",
        "\n",
        "# Recommend movies for a specific user (e.g., user_id = 1)\n",
        "user_id = 1\n",
        "recommended_anime = recommend_anime(user_id, predicted_ratings, full_data, num_recommendations=5)\n",
        "\n",
        "print(\"Recommended Movies for User {}:\".format(user_id))\n",
        "recommended_movies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Conclusions\n",
        "// TODO: write conclusions"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
